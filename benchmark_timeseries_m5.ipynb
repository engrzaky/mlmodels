{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "benchmark_timeseries_m5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engrzaky/mlmodels/blob/master/benchmark_timeseries_m5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDZhO-qF2Pbg",
        "colab_type": "text"
      },
      "source": [
        "# M5 Forecasting Competition GluonTS Template\n",
        "\n",
        "This notebook can be used as a starting point for participating in the M5 forecasting competition using GluonTS-based tooling.\n",
        "M5 Forecasting - Accuracy source image\n",
        "M5 Forecasting - Accuracy\n",
        "Estimate the unit sales of Walmart retail goods\n",
        "Last Updated: 2 months ago\n",
        "About this Competition\n",
        "In the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.\n",
        "Files\n",
        "calendar.csv - Contains information about the dates on which the products are sold.\n",
        "sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n",
        "sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n",
        "sell_prices.csv - Contains information about the price of the products sold per store and date.\n",
        "sales_train_evaluation.csv - Available once month before competition deadline. Will include sales [d_1 - d_1941]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02bsBP7G2Pbk",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olh9xdbx2Pbl",
        "colab_type": "code",
        "outputId": "213a30ba-a2ba-4a32-e465-221b5f31a1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "# %matplotlib inline\n",
        "!pip install pydantic==1.4 --force\n",
        "!pip install mxnet\n",
        "!pip install gluonts\n",
        "import mxnet as mx\n",
        "from mxnet import gluon\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "from pathlib import Path"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydantic==1.4\n",
            "  Using cached https://files.pythonhosted.org/packages/be/9a/a2d9613a70051615a84df6e9d697aad9787ba978bdeb4ad46c754457b3e1/pydantic-1.4-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Collecting dataclasses>=0.6; python_version < \"3.7\"\n",
            "  Using cached https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
            "Installing collected packages: dataclasses, pydantic\n",
            "  Found existing installation: dataclasses 0.7\n",
            "    Uninstalling dataclasses-0.7:\n",
            "      Successfully uninstalled dataclasses-0.7\n",
            "  Found existing installation: pydantic 1.4\n",
            "    Uninstalling pydantic-1.4:\n",
            "      Successfully uninstalled pydantic-1.4\n",
            "Successfully installed dataclasses-0.7 pydantic-1.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydantic"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.4)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: gluonts in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.6/dist-packages (from gluonts) (4.41.1)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.6/dist-packages (from gluonts) (3.2.1)\n",
            "Requirement already satisfied: holidays<0.10,>=0.9 in /usr/local/lib/python3.6/dist-packages (from gluonts) (0.9.12)\n",
            "Requirement already satisfied: ujson~=1.35 in /usr/local/lib/python3.6/dist-packages (from gluonts) (1.35)\n",
            "Requirement already satisfied: pydantic~=1.1 in /usr/local/lib/python3.6/dist-packages (from gluonts) (1.4)\n",
            "Requirement already satisfied: pandas~=1.0 in /usr/local/lib/python3.6/dist-packages (from gluonts) (1.0.3)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.6/dist-packages (from gluonts) (1.18.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->gluonts) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->gluonts) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->gluonts) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->gluonts) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from holidays<0.10,>=0.9->gluonts) (1.12.0)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic~=1.1->gluonts) (0.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas~=1.0->gluonts) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K1OWG6YfHSp",
        "colab_type": "code",
        "outputId": "e5827947-9ebb-41f8-8174-66d3d253444e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfAK8vL82Pb3",
        "colab_type": "text"
      },
      "source": [
        "We also define globally accessible variables, such as the pred length and the input path for the M5 data.\n",
        " Note that single_pred_length corresponds to the length of the val/evaluation periods, while submission_pred_length corresponds to the length of both these periods combined.\n",
        "By default the notebook is configured to run in submission mode (submission will be True), \n",
        "which means that we use all of the data for training and predict new values for a \n",
        "total length of submission_pred_length for which we don't have ground truth values available\n",
        " (performance can be assessed by submitting pred results to Kaggle). \n",
        " In contrast, setting submission to False will instead use the last single_pred_length-many\n",
        "  values of our training set as val points (and hence these values will not be used for training),\n",
        "   which enables us to validate our model's performance offline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyd-PpfB2Pb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_pred_length = 28\n",
        "submission_pred_length = single_pred_length * 2\n",
        "m5_input_path=\"./m5-forecasting-accuracy\"\n",
        "submission=True\n",
        "\n",
        "if submission:\n",
        "    pred_length = submission_pred_length\n",
        "else:\n",
        "    pred_length = single_pred_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwrRv67t2PcJ",
        "colab_type": "text"
      },
      "source": [
        "# Reading the M5 data into GluonTS\n",
        "First we need to convert the provided M5 data into a format that is readable by GluonTS.\n",
        " At this point we assume that the M5 data, which can be downloaded from Kaggle, is present under m5_input_path.\n",
        "MultiVariat Dataset\n",
        "Files\n",
        "calendar.csv               : Contains information about the dates on which the products are sold.\n",
        "sales_train_validation.csv : Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n",
        "sample_submission.csv      : The correct format for submissions. Reference the Evaluation tab for more info.\n",
        "sell_prices.csv            : Contains information about the price of the products sold per store and date.\n",
        "sales_train_evaluation.csv : Available once month before competition deadline. Will include sales [d_1 - d_1941]\n",
        "https://www.kaggle.com/steverab/m5-forecasting-competition-gluonts-template\n",
        "(ID x timeStamp ) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EPvEIkW2PcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calendar               = pd.read_csv(f'{m5_input_path}/calendar.csv')\n",
        "sales_train_val        = pd.read_csv(f'{m5_input_path}/sales_train_validation.csv')\n",
        "sample_submission      = pd.read_csv(f'{m5_input_path}/sample_submission.csv')\n",
        "sell_prices            = pd.read_csv(f'{m5_input_path}/sell_prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfF1CYwZ2Pcb",
        "colab_type": "text"
      },
      "source": [
        "We start the data convertion process by building dynamic features \n",
        "(features that change over time, just like the target values). \n",
        "Here, we are mainly interested in the event indicators event_type_1 and event_type_2. \n",
        "We will mostly drop dynamic time features as GluonTS will automatically add \n",
        "some of these as part of many models' transformation chains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DijsqPya2Pcd",
        "colab_type": "text"
      },
      "source": [
        "# Dynamic Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltq1miMF2Pcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cal_feat = calendar.drop(\n",
        "    ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'd'], \n",
        "    axis=1\n",
        ")\n",
        "cal_feat['event_type_1'] = cal_feat['event_type_1'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
        "cal_feat['event_type_2'] = cal_feat['event_type_2'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
        "test_cal_feat = cal_feat.values.T\n",
        "if submission:\n",
        "    train_cal_feat = test_cal_feat[:,:-submission_pred_length]\n",
        "else:\n",
        "    train_cal_feat = test_cal_feat[:,:-submission_pred_length-single_pred_length]\n",
        "    test_cal_feat  = test_cal_feat[:,:-submission_pred_length]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCc_A_Zb2Pcq",
        "colab_type": "text"
      },
      "source": [
        "List of individual time series   Nb Series x Lenght_time_series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE9rjfaG2Pcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cal_feat_list  = [test_cal_feat] * len(sales_train_val)\n",
        "train_cal_feat_list = [train_cal_feat] * len(sales_train_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZqam_Q2Pcx",
        "colab_type": "text"
      },
      "source": [
        "We then go on to build static features (features which are constant and series-specific).\n",
        " Here, we make use of all categorical features that are provided to us as part of the M5 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761pKLDJ2Pcz",
        "colab_type": "text"
      },
      "source": [
        "# Static Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pR161xj2Pc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_ids                       = sales_train_val[\"state_id\"].astype('category').cat.codes.values\n",
        "state_ids_un , state_ids_counts = np.unique(state_ids, return_counts=True)\n",
        "store_ids                       = sales_train_val[\"store_id\"].astype('category').cat.codes.values\n",
        "store_ids_un , store_ids_counts = np.unique(store_ids, return_counts=True)\n",
        "cat_ids                         = sales_train_val[\"cat_id\"].astype('category').cat.codes.values\n",
        "cat_ids_un , cat_ids_counts     = np.unique(cat_ids, return_counts=True)\n",
        "dept_ids                        = sales_train_val[\"dept_id\"].astype('category').cat.codes.values\n",
        "dept_ids_un , dept_ids_counts   = np.unique(dept_ids, return_counts=True)\n",
        "item_ids                        = sales_train_val[\"item_id\"].astype('category').cat.codes.values\n",
        "item_ids_un , item_ids_counts   = np.unique(item_ids, return_counts=True)\n",
        "##### Static Features \n",
        "static_cat_list          = [item_ids, dept_ids, cat_ids, store_ids, state_ids]\n",
        "static_cat               = np.concatenate(static_cat_list)\n",
        "static_cat               = static_cat.reshape(len(static_cat_list), len(item_ids)).T\n",
        "static_cat_cardinalities = [len(item_ids_un), len(dept_ids_un), len(cat_ids_un), len(store_ids_un), len(state_ids_un)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BsUdb2P2Pc9",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can build both the training and the testing set from target values and both static and dynamic features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1bo7SQi2Pc-",
        "colab_type": "text"
      },
      "source": [
        "# Time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzlRBfda2Pc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.dataset.common import load_datasets, ListDataset\n",
        "from gluonts.dataset.field_names import FieldName\n",
        "#### Remove Categories colum\n",
        "train_df            = sales_train_val.drop([\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"], axis=1)\n",
        "train_target_values = train_df.values\n",
        "if submission == True:\n",
        "    test_target_values = [np.append(ts, np.ones(submission_pred_length) * np.nan) for ts in train_df.values]\n",
        "else:\n",
        "    #### List of individual timeseries\n",
        "    test_target_values  = train_target_values.copy()\n",
        "    train_target_values = [ts[:-single_pred_length] for ts in train_df.values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKhJOvw2PdE",
        "colab_type": "text"
      },
      "source": [
        "Start Dates for each time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxHwLMI02PdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m5_dates = [pd.Timestamp(\"2011-01-29\", freq='1D') for _ in range(len(sales_train_val))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py6V8i4C2PdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = ListDataset([\n",
        "    {\n",
        "        FieldName.TARGET            : target,\n",
        "        FieldName.START             : start,\n",
        "        FieldName.FEAT_DYNAMIC_REAL : fdr,\n",
        "        FieldName.FEAT_STATIC_CAT   : fsc\n",
        "    } for (target, start, fdr, fsc) in zip(train_target_values,   # list of individual time series\n",
        "                                           m5_dates,              # list of start dates\n",
        "                                           train_cal_feat_list,   # List of Dynamic Features\n",
        "                                           static_cat)              # List of Static Features \n",
        "    ],     freq=\"D\")\n",
        "test_ds = ListDataset([\n",
        "    {\n",
        "        FieldName.TARGET            : target,\n",
        "        FieldName.START             : start,\n",
        "        FieldName.FEAT_DYNAMIC_REAL : fdr,\n",
        "        FieldName.FEAT_STATIC_CAT   : fsc\n",
        "    }\n",
        "    for (target, start, fdr, fsc) in zip(test_target_values,\n",
        "                                         m5_dates,\n",
        "                                         test_cal_feat_list,\n",
        "                                         static_cat)\n",
        "], freq=\"D\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbWUKMCV2PdM",
        "colab_type": "text"
      },
      "source": [
        "Just to be sure, we quickly verify that dataset format is correct and that our dataset does indeed \n",
        "contain the correct target values as well as dynamic and static features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qorh7M5I2PdN",
        "colab_type": "code",
        "outputId": "1a5316b4-6b79-4ba6-dbab-85f32630c965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "next(iter(train_ds))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'feat_dynamic_real': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " 'feat_static_cat': array([1437,    3,    1,    0,    0], dtype=int32),\n",
              " 'source': SourceContext(source='list_data', row=0),\n",
              " 'start': Timestamp('2011-01-29 00:00:00', freq='D'),\n",
              " 'target': array([0., 0., 0., ..., 0., 1., 1.], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvoaBugY2PdQ",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0sT0adB2PdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gluonts_create_dynamic(df_dynamic, submission=1, single_pred_length=28, submission_pred_length=10, n_timeseries=1, transpose=1) :\n",
        "    \"\"\"\n",
        "        N_cat x N-timseries\n",
        "    \"\"\"\n",
        "    v = df_dynamic.values.T if transpose else df_dynamic.values\n",
        "\n",
        "    train_cal_feat = v[:,:-submission_pred_length-single_pred_length]\n",
        "    test_cal_feat  = v[:,:-submission_pred_length]\n",
        "\n",
        "    #### List of individual time series   Nb Series x Lenght_time_series\n",
        "    test_list  = [test_cal_feat] * n_timeseries\n",
        "    train_list = [train_cal_feat] * n_timeseries\n",
        "    return train_list, test_list\n",
        "\n",
        "def gluonts_create_static(df_static, submission=1, single_pred_length=28, submission_pred_length=10, n_timeseries=1, transpose=1) :\n",
        "    \"\"\"\n",
        "        N_cat x N-timseries\n",
        "    \"\"\"\n",
        "    ####### Static Features \n",
        "    for col in df_static :\n",
        "      v_col  = df_static[col].astype('category').cat.codes.values\n",
        "      static_cat_list.append(v_col)\n",
        "\n",
        "\n",
        "    static_cat               = np.concatenate(static_cat_list)\n",
        "    static_cat               = static_cat.reshape(len(static_cat_list), n_timeseries).T\n",
        "    # static_cat_cardinalities = [len(df_feature_static[col].unique()) for col in df_feature_static]\n",
        "    return static_cat, static_cat\n",
        "\n",
        "\n",
        "def gluonts_create_timeseries(df_timeseries, submission=1, single_pred_length=28, submission_pred_length=10, n_timeseries=1, transpose=1) :\n",
        "    \"\"\"\n",
        "        N_cat x N-timseries\n",
        "    \"\"\"\n",
        "    #### Remove Categories colum\n",
        "    train_target_values = df_timeseries.values\n",
        "\n",
        "    if submission == True:\n",
        "        test_target_values = [np.append(ts, np.ones(submission_pred_length) * np.nan) for ts in df_timeseries.values]\n",
        "\n",
        "\n",
        "    else:\n",
        "        #### List of individual timeseries\n",
        "        test_target_values  = train_target_values.copy()\n",
        "        train_target_values = [ts[:-single_pred_length] for ts in df_timeseries.values]\n",
        "\n",
        "    return train_target_values, test_target_values\n",
        "\n",
        "\n",
        "#### Start Dates for each time series\n",
        "def create_startdate(date=\"2011-01-29\", freq=\"1D\", n_timeseries=1):\n",
        "   start_dates_list = [pd.Timestamp(date, freq=freq) for _ in range(n_timeseries)]\n",
        "   return start_dates_list\n",
        "\n",
        "\n",
        "def gluonts_create_dataset(train_timeseries_list, start_dates_list, train_dynamic_list,  train_static_list, freq=\"D\" ) :\n",
        "    from gluonts.dataset.common import load_datasets, ListDataset\n",
        "    from gluonts.dataset.field_names import FieldName\n",
        "    train_ds = ListDataset([\n",
        "        {\n",
        "            FieldName.TARGET            : target,\n",
        "            FieldName.START             : start,\n",
        "            FieldName.FEAT_DYNAMIC_REAL : fdr,\n",
        "            FieldName.FEAT_STATIC_CAT   : fsc\n",
        "        } for (target, start, fdr, fsc) in zip(train_timeseries_list,   # list of individual time series\n",
        "                                               start_dates_list,              # list of start dates\n",
        "                                               train_dynamic_list,   # List of Dynamic Features\n",
        "                                               train_static_list)              # List of Static Features \n",
        "        ],     freq=freq)\n",
        "    return train_ds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeqsNZDg2PdU",
        "colab_type": "text"
      },
      "source": [
        "# Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHfijsu-2PdU",
        "colab_type": "code",
        "outputId": "31e936e1-d641-473c-91c2-46cbf8ef7857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "n_timeseries           = len(sales_train_val)\n",
        "single_pred_length     = 28\n",
        "submission_pred_length = single_pred_length * 2\n",
        "startdate              = \"2011-01-29\"\n",
        "freq                   = \"1D\"\n",
        "submission= 0\n",
        "\n",
        "cal_feat = calendar.drop( ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'd'],  axis=1 )\n",
        "cal_feat['event_type_1'] = cal_feat['event_type_1'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
        "cal_feat['event_type_2'] = cal_feat['event_type_2'].apply(lambda x: 0 if str(x)==\"nan\" else 1)\n",
        "\n",
        "\n",
        "\n",
        "df_dynamic    = cal_feat\n",
        "df_static     = sales_train_val[\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"]\n",
        "#df_static     = sales_train_val[[\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"]] # This works but another problem arises (Zahoor)\n",
        "df_timeseries = sales_train_val.drop([\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"], axis=1)\n",
        "\n",
        "\n",
        "def pandas_to_gluonts_multiseries(df_timeseries, df_dynamic, df_static, pars=None) :\n",
        "\n",
        "    submission             = pars['submission']\n",
        "    single_pred_length     = pars['single_pred_length']\n",
        "    submission_pred_length = pars['submission_pred_length']\n",
        "    n_timeseries           = pars['n_timeseries']\n",
        "    start_date             = pars['start_date']\n",
        "    train_dynamic_list, test_dynamic_list       = gluonts_create_dynamic(df_dynamic, submission=submission, single_pred_length=single_pred_length, \n",
        "                                                                         submission_pred_length=submission_pred_length, n_timeseries=n_timeseries, transpose=1)\n",
        "\n",
        "    train_static_list, test_static_list          = gluonts_create_static(df_static , submission=submission, single_pred_length=single_pred_length, \n",
        "                                                                         submission_pred_length=submission_pred_length, n_timeseries=n_timeseries, transpose=0)\n",
        "    train_timeseries_list, test_timeseries_list = gluonts_create_timeseries(df_timeseries, submission=submission, single_pred_length=single_pred_length, \n",
        "                                                                            submission_pred_length=submission_pred_length, n_timeseries=n_timeseries, transpose=0)\n",
        "\n",
        "    start_dates_list = create_startdate(date=start_date, freq=freq, n_timeseries=1)\n",
        "    train_ds = gluonts_create_dataset(train_timeseries_list, start_dates_list, train_dynamic_list, train_static_list, freq=freq ) \n",
        "    test_ds  = gluonts_create_dataset(test_timeseries_list,  start_dates_list, test_dynamic_list,  test_static_list,  freq=freq )     \n",
        "    return train_ds, test_ds\n",
        "\n",
        "train_ds, test_ds = pandas_to_gluonts_multiseries(df_timeseries, df_dynamic, df_static, pars=None) "
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('item_id', 'dept_id', 'cat_id', 'store_id', 'state_id')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-ec8ad0b7f727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_dynamic\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mcal_feat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_static\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0msales_train_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dept_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cat_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"state_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#df_static     = sales_train_val[[\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"]] # This works but another problem arises (Zahoor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdf_timeseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales_train_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"item_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dept_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cat_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"state_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('item_id', 'dept_id', 'cat_id', 'store_id', 'state_id')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tka4Fph32PdX",
        "colab_type": "text"
      },
      "source": [
        "# Define the estimator\n",
        "Having obtained our training and testing data, we can now create a GluonTS estimator. In our example we will use the DeepAREstimator, an autoregressive RNN which was developed primarily for the purpose of time series forecasting. Note however that you can use a variety of different estimators. Also, since GluonTS is mainly target at probabilistic time series forecasting, lots of different output distributions can be specified. In the M5 case, we think that the NegativeBinomialOutput distribution best describes the output.\n",
        "For a full list of available estimators and possible initialization arguments see https://gluon-ts.mxnet.io/api/gluonts/gluonts.model.html.\n",
        "For a full list of available output distributions and possible initialization arguments see https://gluon-ts.mxnet.io/api/gluonts/gluonts.distribution.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-tV_4RV2PdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.model.deepar import DeepAREstimator\n",
        "from gluonts.distribution.neg_binomial import NegativeBinomialOutput\n",
        "from gluonts.trainer import Trainer\n",
        "\n",
        "estimator = DeepAREstimator(\n",
        "    pred_length     = pred_length,\n",
        "    freq                  = \"D\",\n",
        "    distr_output          = NegativeBinomialOutput(),\n",
        "    use_feat_dynamic_real = True,\n",
        "    use_feat_static_cat   = True,\n",
        "    cardinality           = static_cat_cardinalities,\n",
        "    trainer               = Trainer(\n",
        "    learning_rate         = 1e-3,\n",
        "    epochs                = 100,\n",
        "    num_batches_per_epoch = 50,\n",
        "    batch_size            = 32\n",
        "    )\n",
        ")\n",
        "\n",
        "predictor = estimator.train(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL1ktJ312Pda",
        "colab_type": "text"
      },
      "source": [
        "# Generating forecasts\n",
        "Once the estimator is fully trained, we can generate preds from it for the test values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJIxHYl12Pda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gluonts.evaluation.backtest import make_evaluation_preds\n",
        "\n",
        "forecast_it, ts_it = make_evaluation_preds(\n",
        "    dataset=test_ds,\n",
        "    predictor=predictor,\n",
        "    num_samples=100\n",
        ")\n",
        "\n",
        "print(\"Obtaining time series conditioning values ...\")\n",
        "tss = list(tqdm(ts_it, total=len(test_ds)))\n",
        "print(\"Obtaining time series preds ...\")\n",
        "forecasts = list(tqdm(forecast_it, total=len(test_ds)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7WlWOv2Pdd",
        "colab_type": "text"
      },
      "source": [
        "# Local performance val (if submission is False)\n",
        "Since we don't want to constantly submit our results to Kaggle, it is important to being able to evaluate performace on our own val set offline. To do so, we create a custom evaluator which, in addition to GluonTS's standard performance metrics, also returns MRMSSE (corresponding to the mean RMSSE). Note that the official score for the M5 competition, the WRMSSE, is not yet computed. A future version of this notebook will replace the MRMSSE by the WRMSSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx9oc5Bh2Pdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if submission == False:\n",
        "    \n",
        "    from gluonts.evaluation import Evaluator\n",
        "    \n",
        "    class M5Evaluator(Evaluator):\n",
        "        \n",
        "        def get_metrics_per_ts(self, time_series, forecast):\n",
        "            successive_diff  = np.diff(time_series.values.reshape(len(time_series)))\n",
        "            successive_diff  = successive_diff ** 2\n",
        "            successive_diff  = successive_diff[:-pred_length]\n",
        "            denom            = np.mean(successive_diff)\n",
        "            pred_values      = forecast.samples.mean(axis=0)\n",
        "            true_values      = time_series.values.reshape(len(time_series))[-pred_length:]\n",
        "            num              = np.mean((pred_values - true_values)**2)\n",
        "            rmsse            = num / denom\n",
        "            metrics          = super().get_metrics_per_ts(time_series, forecast)\n",
        "            metrics[\"RMSSE\"] = rmsse\n",
        "            return metrics\n",
        "        \n",
        "        def get_aggregate_metrics(self, metric_per_ts):\n",
        "            wrmsse = metric_per_ts[\"RMSSE\"].mean()\n",
        "            agg_metric , _ = super().get_aggregate_metrics(metric_per_ts)\n",
        "            agg_metric[\"MRMSSE\"] = wrmsse\n",
        "            return agg_metric, metric_per_ts\n",
        "        \n",
        "    \n",
        "    evaluator = M5Evaluator(quantiles=[0.5, 0.67, 0.95, 0.99])\n",
        "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
        "    print(json.dumps(agg_metrics, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW5yDmyX2Pdg",
        "colab_type": "text"
      },
      "source": [
        "# Converting forecasts back to M5 submission format (if submission is True)\n",
        "Since GluonTS estimators return a sample-based probabilistic forecasting predictor, we first need to reduce these results to a single pred per time series. This can be done by computing the mean or median over the predicted sample paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icmt6l9I2Pdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if submission == True:\n",
        "    forecasts_acc = np.zeros((len(forecasts), pred_length))\n",
        "    for i in range(len(forecasts)):\n",
        "        forecasts_acc[i] = np.mean(forecasts[i].samples, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_5J6472Pdj",
        "colab_type": "text"
      },
      "source": [
        "We then reshape the forecasts into the correct data shape for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9HX-n092Pdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if submission == True:\n",
        "    forecasts_acc_sub = np.zeros((len(forecasts)*2, single_pred_length))\n",
        "    forecasts_acc_sub[:len(forecasts)] = forecasts_acc[:,:single_pred_length]\n",
        "    forecasts_acc_sub[len(forecasts):] = forecasts_acc[:,single_pred_length:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I733nByv2Pdm",
        "colab_type": "text"
      },
      "source": [
        "And verfiy that reshaping is consistent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ypAgHf2Pdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if submission == True:\n",
        "    np.all(np.equal(forecasts_acc[0], np.append(forecasts_acc_sub[0], forecasts_acc_sub[30490])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho37K-dG2Pdq",
        "colab_type": "text"
      },
      "source": [
        "Then, we save our submission into a timestamped CSV file which can subsequently be uploaded to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_9J08uP2Pdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if submission == True:\n",
        "    import time\n",
        "    sample_submission            = pd.read_csv(f'{m5_input_path}/sample_submission.csv')\n",
        "    sample_submission.iloc[:,1:] = forecasts_acc_sub\n",
        "    submission_id                = 'submission_{}.csv'.format(int(time.time()))\n",
        "    sample_submission.to_csv(submission_id, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6kdZTqk2Pdt",
        "colab_type": "text"
      },
      "source": [
        "# Plotting sample preds\n",
        "Finally, we can also visualize our preds for some of the time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf6BA5G62Pdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_log_path = \"./plots/\"\n",
        "directory = os.path.dirname(plot_log_path)\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "    \n",
        "def plot_prob_forecasts(ts_entry, forecast_entry, path, sample_id, inline=True):\n",
        "    plot_length = 150\n",
        "    pred_intervals = (50, 67, 95, 99)\n",
        "    legend = [\"observations\", \"median pred\"] + [f\"{k}% pred interval\" for k in pred_intervals][::-1]\n",
        "\n",
        "    _, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
        "    ts_entry[-plot_length:].plot(ax=ax)\n",
        "    forecast_entry.plot(pred_intervals=pred_intervals, color='g')\n",
        "    ax.axvline(ts_entry.index[-pred_length], color='r')\n",
        "    plt.legend(legend, loc=\"upper left\")\n",
        "    if inline:\n",
        "        plt.show()\n",
        "        plt.clf()\n",
        "    else:\n",
        "        plt.savefig('{}forecast_{}.pdf'.format(path, sample_id))\n",
        "        plt.close()\n",
        "\n",
        "print(\"Plotting time series preds ...\")\n",
        "for i in tqdm(range(5)):\n",
        "    ts_entry = tss[i]\n",
        "    forecast_entry = forecasts[i]\n",
        "    plot_prob_forecasts(ts_entry, forecast_entry, plot_log_path, i)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}